{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 - load the training data locally #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import linear_model \n",
    "from sklearn.externals import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import azureml\n",
    "from azureml.core import Run\n",
    "from azureml.core import Workspace\n",
    "from azureml.core.run import Run\n",
    "from azureml.core.experiment import Experiment\n",
    "import pickle\n",
    "\n",
    "print(\"Current working directory is \", os.path.abspath(os.path.curdir))\n",
    "df_affordability = pd.read_csv('./data/UsedCars_Affordability.csv', delimiter=',')\n",
    "print(df_affordability.head())\n",
    "\n",
    "full_X = df_affordability[[\"Age\", \"KM\"]]\n",
    "full_Y = df_affordability[[\"Affordable\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 - Define a helper method for training, evaluating and registering#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_register_model(experiment_name, full_X, full_Y,training_set_percentage):\n",
    "\n",
    "    # start a training run by defining an experiment\n",
    "    myexperiment = Experiment(ws, experiment_name)\n",
    "    run = myexperiment.start_logging()\n",
    "\n",
    "\n",
    "    train_X, test_X, train_Y, test_Y = train_test_split(full_X, full_Y, train_size=training_set_percentage, \n",
    "                                                        random_state=42)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(train_X)\n",
    "    clf = linear_model.LogisticRegression(C=1)\n",
    "    clf.fit(X_scaled, train_Y)\n",
    "\n",
    "    scaled_inputs = scaler.transform(test_X)\n",
    "    predictions = clf.predict(scaled_inputs)\n",
    "    score = accuracy_score(test_Y, predictions)\n",
    "\n",
    "    print(\"With %0.2f percent of data, model accuracy reached %0.4f.\" % (training_set_percentage, score))\n",
    "\n",
    "    # Log the training metrics to Azure Machine Learning service run history\n",
    "    run.log(\"Training_Set_Percentage\", training_set_percentage)\n",
    "    run.log(\"Accuracy\", score)\n",
    "    run.complete()\n",
    "\n",
    "    # Save the model to your local outputs directory\n",
    "    model_name = experiment_name + '.pkl'\n",
    "    output_model_path = './outputs/' + model_name\n",
    "    pickle.dump(clf,open(output_model_path,'wb'))\n",
    "    \n",
    "    # Upload and register this version of the model with Azure Machine Learning service\n",
    "    destination_path = 'outputs/' + model_name\n",
    "    run.upload_file(destination_path, output_model_path) # destination, source\n",
    "    registered_model = run.register_model(model_name='usedcarsmodel', model_path=destination_path)\n",
    "\n",
    "    print(registered_model.name, registered_model.id, registered_model.version, sep = '\\t')\n",
    "\n",
    "    return (clf, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 - Run a few experiments in your Azure ML Workspace #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify AML SDK Installed\n",
    "print(\"SDK Version:\", azureml.core.VERSION)\n",
    "\n",
    "\n",
    "# Create a new Workspace or retrieve the existing one\n",
    "#Provide the Subscription ID of your existing Azure subscription\n",
    "subscription_id = \"xxx-xxx-xxx\"\n",
    "\n",
    "#Provide values for the Resource Group and Workspace that will be created\n",
    "resource_group = \"aml-workspace-z\"\n",
    "workspace_name = \"aml-workspace-z\"\n",
    "workspace_region = 'westcentralus' # eastus, westcentralus, southeastasia, australiaeast, westeurope\n",
    "\n",
    "# By using the exist_ok param, if the worskpace already exists we get a reference to the existing workspace instead of an error\n",
    "ws = Workspace.create(\n",
    "    name = workspace_name,\n",
    "    subscription_id = subscription_id,\n",
    "    resource_group = resource_group, \n",
    "    location = workspace_region,\n",
    "    exist_ok = True)\n",
    "\n",
    "print(\"Workspace Provisioning complete.\")\n",
    "\n",
    "\n",
    "# Create an experiment, log metrics and register the created models for multiple training runs\n",
    "experiment_name = \"Experiment-02-01\"\n",
    "training_set_percentage = 0.25\n",
    "model, score = train_eval_register_model(experiment_name, full_X, full_Y, training_set_percentage)\n",
    "\n",
    "experiment_name = \"Experiment-02-02\"\n",
    "training_set_percentage = 0.5\n",
    "model, score = train_eval_register_model(experiment_name, full_X, full_Y, training_set_percentage)\n",
    "\n",
    "experiment_name = \"Experiment-02-03\"\n",
    "training_set_percentage = 0.75\n",
    "model, score = train_eval_register_model(experiment_name, full_X, full_Y, training_set_percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 - Query for all Experiments #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can retreive the list of all experiments in Workspace using the following:\n",
    "all_experiments = ws.experiments\n",
    "\n",
    "print(all_experiments)\n",
    "\n",
    "# Query for the metrics of a particular experiment\n",
    "# You can retrieve an existing experiment by constructing an Experiment object using the name of an existing experiment.\n",
    "my_experiment = Experiment(ws, \"Experiment-02-03\")\n",
    "print(my_experiment)\n",
    "\n",
    "# Query an experiment for metrics\n",
    "# With an experiment in hand, you retrieve any metrics collected for any of its child runs \n",
    "my_experiment_runs = my_experiment.get_runs()\n",
    "print( [ (run.experiment.name, run.id, run.get_metrics()) for run in my_experiment_runs] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5 - Submit an experiment to Azure Batch AI and log metrics for multiple training runs #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"UsedCars_Batch_02\"\n",
    "\n",
    "from azureml.core import Experiment\n",
    "exp = Experiment(workspace=ws, name=experiment_name)\n",
    "\n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "import os\n",
    "\n",
    "# choose a name for your cluster\n",
    "batchai_cluster_name = \"carscluster02\"\n",
    "cluster_min_nodes = 1\n",
    "cluster_max_nodes = 3\n",
    "vm_size = \"STANDARD_DS11_V2\"\n",
    "\n",
    "if batchai_cluster_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[batchai_cluster_name]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print('Found existing compute target, using this compute target instead of creating:  ' + \n",
    "              batchai_cluster_name)\n",
    "else:\n",
    "    print('Creating a new compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = vm_size,  \n",
    "                                                                vm_priority = 'lowpriority', # optional\n",
    "                                                                min_nodes = cluster_min_nodes, \n",
    "                                                                max_nodes = cluster_max_nodes)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, batchai_cluster_name, provisioning_config)\n",
    "    \n",
    "    # can poll for a minimum number of nodes and for a specific timeout. \n",
    "    # if no min node count is provided it will use the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "    \n",
    "     # For a more detailed view of current BatchAI cluster status, use the 'status' property    \n",
    "    print(compute_target.status.serialize())\n",
    "\n",
    "# Upload the dataset to the DataStore\n",
    "ds = ws.get_default_datastore()\n",
    "print(ds.datastore_type, ds.account_name, ds.container_name)\n",
    "ds.upload(src_dir='./data', target_path='used_cars', overwrite=True, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare batch training script\n",
    "# - See ./training/train.py\n",
    "\n",
    "\n",
    "# Create estimator\n",
    "from azureml.train.estimator import Estimator\n",
    "\n",
    "script_params = {\n",
    "    '--data-folder': ds.as_mount(),\n",
    "    '--training-set-percentage': 0.3\n",
    "}\n",
    "\n",
    "est_config = Estimator(source_directory='./training',\n",
    "                script_params=script_params,\n",
    "                compute_target=compute_target,\n",
    "                entry_script='train.py',\n",
    "                conda_packages=['scikit-learn','pandas'])\n",
    "\n",
    "# Execute the job\n",
    "run = exp.submit(config=est_config)\n",
    "\n",
    "# Poll for job status\n",
    "run.wait_for_completion(show_output=True) # value of True will display a verbose, streaming log\n",
    "\n",
    "print(run.get_file_names())\n",
    "\n",
    "# Register this version of the model with Azure Machine Learning service\n",
    "registered_model = run.register_model(model_name='usedcarsmodel', model_path='outputs/model.pkl')\n",
    "\n",
    "print(registered_model.name, registered_model.id, registered_model.version, sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6 - Retrieve the metrics for the model trained in Azure Batch AI #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the recorded metrics from the run\n",
    "print(run.get_metrics())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (azure_automl)",
   "language": "python",
   "name": "azure_automl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
